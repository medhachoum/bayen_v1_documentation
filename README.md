# Advanced Saudi Legal Assistant API

Welcome to the Advanced Saudi Legal Assistant API. This API provides expert legal responses tailored to Saudi Arabian laws and regulations. It is designed to accept a full conversation context, choose an appropriate AI model based on your selection, and stream back responses incrementally. Each response includes a fixed conversation title generated once from the initial query, along with additional metadata.

> **Note:** The API response is delivered via Server-Sent Events (SSE). Clients must be capable of handling streaming data.

---

## Base URL

All API requests should be made to the following base URL:

```
https://bayen-v1.onrender.com
```

---

## Authentication

Every request to the API requires a valid API key. Use the custom HTTP header:

- **X-API-Key**: Your project-specific API key.

> **Security Note:** Ensure that your API key is kept confidential and never exposed on the client side.

---

## Endpoints

### POST `/chat`

This endpoint accepts a conversation context and returns a streaming response with the AI-generated legal answer, along with detailed metadata. 

#### Summary

- **Method:** `POST`
- **Path:** `/chat`
- **Tags:** Chat
- **Description:**  
  This endpoint processes a complete conversation and returns an incremental, streamed response. The response includes:
  - **message:** The AI’s answer (delivered as chunks).
  - **citations:** A list of reference texts (if available).
  - **metadata:** Detailed information including a unique response identifier, creation timestamp, a fixed conversation title, and the model alias provided by the client.
  
  The conversation title is generated based on the initial user query and remains unchanged for subsequent messages in the same session.

---

## Request Format

All requests must be in JSON format. The following schema details the required structure.

### ChatRequest

| Parameter           | Type     | Required | Description |
| ------------------- | -------- | -------- | ----------- |
| **model**           | `string` | Yes      | The model alias to be used for the request. Accepted values are: `bayen-pro` or `bayen-lite`. The provided alias will be reflected back in the response metadata. |
| **messages**        | `array`  | Yes      | An ordered list of message objects representing the conversation. Each message must include a role and content. If no system message is provided, a default system prompt is automatically inserted. |
| **structured_output** | `boolean` | Optional (default: `true`) | Specifies whether the output should be structured following a predefined JSON Schema. Set to `true` by default. |
| **max_tokens**      | `integer` | Optional | The maximum number of tokens (words/characters) for the response. This field is included only if explicitly provided. |

### Message Object

| Field    | Type     | Required | Description |
| -------- | -------- | -------- | ----------- |
| **role** | `string` (enum: "system", "user", "assistant") | Yes      | The role of the message in the conversation. Use `system` for directives, `user` for queries, and `assistant` for previous AI responses. |
| **content** | `string` | Yes      | The text content of the message. |

---

## Example Request

Below is an example using cURL:

```bash
curl --request POST \
  --url https://bayen-v1.onrender.com/chat \
  --header "X-API-Key: YOUR_API_KEY" \
  --header "Content-Type: application/json" \
  --header "Accept: text/event-stream" \
  --data '{
    "model": "bayen-pro",
    "messages": [
      {
        "role": "user",
        "content": "What are the penalties for forgery under Saudi law?"
      }
    ],
    "structured_output": true,
    "max_tokens": 500
  }'
```

Replace `YOUR_API_KEY` with your valid API key.

---

## Response Specification

The API returns a streaming response (SSE) with each chunk containing a JSON object structured as follows:

### Response JSON Schema

| Field       | Type     | Description |
| ----------- | -------- | ----------- |
| **message** | `string` | A portion of the complete answer generated by the AI, delivered incrementally. |
| **citations** | `array` of `string` | A list of citation URLs or references (if available). |
| **metadata** | `object` | Additional information about the response including: |
| &nbsp;&nbsp;&nbsp;&nbsp;**id**      | `string`  | A unique identifier for the response chunk. |
| &nbsp;&nbsp;&nbsp;&nbsp;**model**   | `string`  | The model alias that was provided in the request (e.g., `bayen-pro` or `bayen-lite`). |
| &nbsp;&nbsp;&nbsp;&nbsp;**created** | `number`  | Unix timestamp (in seconds) marking when the response was created. |
| &nbsp;&nbsp;&nbsp;&nbsp;**object**  | `string`  | The object type (e.g., `chat.completion`). |
| &nbsp;&nbsp;&nbsp;&nbsp;**title**   | `string`  | A brief, fixed title for the conversation generated from the initial query. This title remains constant for the session. |

### Streaming Behavior

- **Data Format:**  
  Each data chunk is prefixed with `data: ` and separated by two newline characters (`\n\n`).

- **Completion Indicator:**  
  The stream ends when the API sends `data: [DONE]`.

---

## Detailed Flow

1. **Request Handling:**  
   - The API accepts the full conversation context. If a system message is not provided, it automatically prepends the default system prompt.
   - The `model` field lets the client choose the desired model configuration. Internally, this alias is mapped to the appropriate configuration, but the original alias is retained in the response metadata.
  
2. **Payload Generation:**  
   - The payload includes configuration parameters for streaming responses, search filters, temperature, nucleus sampling (top_p), and penalty values.
   - The optional `max_tokens` is included only if specified by the client.
   - Structured output is enabled by default to ensure that responses match the defined JSON schema.
  
3. **Streaming Response:**  
   - The API returns a streamed response where each chunk contains part of the generated answer.
   - Metadata is updated as new chunks arrive. The conversation title is generated once from the initial query (the model is instructed via the system prompt) and then remains fixed.
   - The response is delivered via SSE, ending with `data: [DONE]`.

4. **Response Assembly:**  
   - The client should reassemble the chunks to display the complete answer.
   - Metadata in each chunk reflects the chosen model (as provided by the client) rather than the internal configuration.

5. **Output Details:**  
   - In some response chunks, you may notice a segment enclosed within `<think> ... </think>`. This section contains the internal reasoning process of the AI model before finalizing the answer. This is included to provide transparency about the model's chain-of-thought, though it is not intended for end-user display.

---

## Example Response

An example stream of data chunks might look like this:

```json
data: {
  "message": "The penalties for forgery include ...",
  "citations": [],
  "metadata": {
    "model": "bayen-pro",
    "id": "a2f2ce2d-7b7e-48f8-8b56-4509d39e1937",
    "created": 1743529939,
    "object": "chat.completion",
    "title": "Penalties for Forgery under Saudi Law"
  }
}
data: {
  "message": " additional details about imprisonment and fines ...",
  "citations": [],
  "metadata": {
    "model": "bayen-pro",
    "id": "a2f2ce2d-7b7e-48f8-8b56-4509d39e1937",
    "created": 1743529939,
    "object": "chat.completion",
    "title": "Penalties for Forgery under Saudi Law"
  }
}
data: [DONE]
```

---

## Usage Examples in Client Applications

### Python Client Example

Below is a sample Python script using `httpx` to consume the streaming API:

```python
import asyncio
import httpx
import json

async def main():
    url = "https://bayen-v1.onrender.com/chat"
    headers = {
        "X-API-Key": "YOUR_API_KEY",
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }
    payload = {
        "model": "bayen-pro",
        "messages": [
            {"role": "user", "content": "What are the penalties for forgery under Saudi law?"}
        ],
        "structured_output": True,
        "max_tokens": 500
    }
    
    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream("POST", url, headers=headers, json=payload) as response:
            async for line in response.aiter_lines():
                if not line.strip():
                    continue
                if line.strip() == "[DONE]":
                    break
                # Remove "data: " prefix
                content = line.strip()[6:]
                try:
                    chunk = json.loads(content)
                    print("Chunk message:", chunk.get("message"))
                    print("Chunk metadata:", chunk.get("metadata"))
                except Exception as e:
                    print("Error parsing chunk:", line, e)

asyncio.run(main())
```

> **Note:** Replace `YOUR_API_KEY` with your actual API key.

---

## FAQ

### 1. How do I send a complete conversation context to the API?
- **Answer:**  
  You must send all previous messages in the conversation in the `messages` array. This includes user messages and any system or assistant messages. If you omit a system message, the API automatically prepends a default system prompt to set context.

### 2. How is the model chosen?
- **Answer:**  
  The API accepts a `model` parameter with values such as `bayen-pro` or `bayen-lite`. Internally, the alias is mapped to the appropriate configuration. However, the response metadata always returns the exact model alias provided in the request for clarity.

### 3. What is the `max_tokens` parameter used for?
- **Answer:**  
  The `max_tokens` parameter controls the maximum length of the generated response. It is only included in the request payload if explicitly provided. Otherwise, it is omitted, and the API uses its default settings.

### 4. How is the conversation title generated and maintained?
- **Answer:**  
  The AI is instructed via the system prompt to generate a brief conversation title based on the initial user query. This title is included in the `metadata` (under the `title` key) in every streamed chunk, ensuring that it remains constant throughout the conversation.

### 5. What is the significance of the `<think> ... </think>` section in the response?
- **Answer:**  
  Within the final answer, there is a section enclosed between `<think>` tags that represents the AI’s internal reasoning or chain-of-thought prior to finalizing its response. This information is intended for transparency and debugging purposes. It is generally not meant for end-user display but can be used by developers to understand the AI's decision-making process.

### 6. How should I handle the streaming response?
- **Answer:**  
  The API sends data as Server-Sent Events (SSE). Each chunk is prefixed with `data: ` and ends with a blank line. The stream terminates when a chunk with `data: [DONE]` is received. Clients must implement SSE or a similar mechanism to reassemble the full response.

### 7. What are the best practices for maintaining context in subsequent requests?
- **Answer:**  
  Always include the full conversation history (all previous messages) when sending new requests so that the AI can maintain context. The conversation title remains fixed after the first response, so reuse it in the metadata if necessary on the client side.

### 8. How do I ensure secure communication with the API?
- **Answer:**  
  Always include your API key in the `X-API-Key` header. Secure storage and usage of the API key are vital. Do not expose this key in client-side code.

---

## Additional Information

For further assistance or any inquiries regarding the API, please refer to this documentation or contact the development team through the project's repository.

---

*End of Documentation*


